"""
Tests unitaires pour les mod√®les de Machine Learning
"""

import pandas as pd
import numpy as np
import sys
import os

# Ajouter le dossier racine au path Python
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_data_preparation():
    """Test de la pr√©paration des donn√©es pour les mod√®les."""
    # Cr√©er des donn√©es de test
    data = {
        'GoalDiff': [1, -1, 2, 0, -2],
        'ShotDiff': [3, -2, 5, 1, -3],
        'ResultCode': [1, -1, 1, 0, -1]
    }
    
    df = pd.DataFrame(data)
    
    # V√©rifier les types de donn√©es
    assert df['GoalDiff'].dtype in [np.int64, np.float64]
    assert df['ResultCode'].dtype in [np.int64, np.float64]
    
    # V√©rifier la distribution de la target
    target_counts = df['ResultCode'].value_counts()
    assert len(target_counts) == 3  # -1, 0, 1
    
    print("‚úÖ Test de pr√©paration des donn√©es r√©ussi")

def test_train_test_split():
    """Test du split train/test."""
    np.random.seed(42)
    
    # Cr√©er des donn√©es
    n_samples = 100
    X = np.random.randn(n_samples, 5)
    y = np.random.choice([-1, 0, 1], n_samples, p=[0.3, 0.25, 0.45])
    
    # Simuler un split 80/20
    split_idx = int(n_samples * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    assert len(X_train) == 80
    assert len(X_test) == 20
    assert len(y_train) == 80
    assert len(y_test) == 20
    
    print("‚úÖ Test de train/test split r√©ussi")

def test_model_metrics():
    """Test des m√©triques d'√©valuation."""
    # Donn√©es de test
    y_true = np.array([1, 0, -1, 1, 0])
    y_pred = np.array([1, 0, 0, 1, -1])
    
    # Calculer l'accuracy manuellement
    correct = (y_true == y_pred).sum()
    total = len(y_true)
    accuracy = correct / total
    
    assert accuracy == 0.6  # 3 corrects sur 5
    print("‚úÖ Test des m√©triques d'√©valuation r√©ussi")

def test_confusion_matrix():
    """Test de la matrice de confusion."""
    y_true = np.array([1, 0, -1, 1, 0, -1])
    y_pred = np.array([1, 0, 0, 1, -1, -1])
    
    # Calculer la matrice de confusion manuellement
    classes = [-1, 0, 1]
    cm = np.zeros((3, 3), dtype=int)
    
    for true, pred in zip(y_true, y_pred):
        i = classes.index(true)
        j = classes.index(pred)
        cm[i, j] += 1
    
    # V√©rifier avec sklearn (si disponible)
    try:
        from sklearn.metrics import confusion_matrix
        sk_cm = confusion_matrix(y_true, y_pred, labels=classes)
        # V√©rifier que notre calcul correspond √† sklearn
        assert np.array_equal(cm, sk_cm)
    except ImportError:
        # Si sklearn n'est pas disponible, v√©rifier manuellement
        assert cm[0, 0] == 1  # -1 pr√©dit comme -1
        assert cm[0, 1] == 1  # -1 pr√©dit comme 0
        assert cm[1, 1] == 1  # 0 pr√©dit comme 0
        assert cm[1, 0] == 1  # 0 pr√©dit comme -1
        assert cm[2, 2] == 2  # 1 pr√©dit comme 1
    
    print("‚úÖ Test de la matrice de confusion r√©ussi")

def test_feature_importance():
    """Test de l'importance des features."""
    # Simuler des importances de features
    features = ['GoalDiff', 'ShotDiff', 'CornerDiff', 'HomeForm', 'AwayForm']
    importances = np.array([0.3, 0.25, 0.2, 0.15, 0.1])
    
    # V√©rifier que les importances somment √† 1 (ou proche)
    total_importance = importances.sum()
    assert 0.99 <= total_importance <= 1.01
    
    # V√©rifier que la feature la plus importante est GoalDiff
    most_important_idx = importances.argmax()
    assert features[most_important_idx] == 'GoalDiff'
    
    print("‚úÖ Test de l'importance des features r√©ussi")

def test_model_comparison():
    """Test de la comparaison de mod√®les."""
    # Simuler les performances de diff√©rents mod√®les
    models = {
        'Logistic Regression': 0.582,
        'Random Forest': 0.625,
        'XGBoost': 0.648
    }
    
    # V√©rifier que XGBoost est le meilleur
    best_model = max(models, key=models.get)
    best_accuracy = models[best_model]
    
    assert best_model == 'XGBoost'
    assert best_accuracy == 0.648
    assert best_accuracy > 0.6  # Doit √™tre meilleur que le baseline
    
    print("‚úÖ Test de comparaison de mod√®les r√©ussi")

def run_all_tests():
    """Ex√©cute tous les tests."""
    print("\n" + "="*50)
    print("üß™ LANCEMENT DES TESTS DES MOD√àLES")
    print("="*50)
    
    tests = [
        test_data_preparation,
        test_train_test_split,
        test_model_metrics,
        test_confusion_matrix,
        test_feature_importance,
        test_model_comparison
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            test()
            passed += 1
        except AssertionError as e:
            print(f"‚ùå {test.__name__} √©chou√© : {e}")
            failed += 1
        except Exception as e:
            print(f"‚ùå {test.__name__} erreur : {e}")
            failed += 1
    
    print("\n" + "="*50)
    print(f"üìä R√âSULTATS : {passed} tests r√©ussis, {failed} tests √©chou√©s")
    print("="*50)
    
    return passed, failed

if __name__ == "__main__":
    run_all_tests()